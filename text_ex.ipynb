{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRC6AOtii/NKMS7berDbUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiasolanki/TextAnalysis/blob/main/text_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKnXoi5ZskF8",
        "outputId": "e66956fd-b325-4e20-e1b9-0c3547e34d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello sample text\n"
          ]
        }
      ],
      "source": [
        "noise=[\"is\",\"a\",\"this\",\"...\"]\n",
        "def remove_noise(input_text):\n",
        "  words=input_text.split()\n",
        "  noise_free_words=[word for word in words if word not in noise]\n",
        "  noise_free_text=\" \".join(noise_free_words)\n",
        "  return noise_free_text\n",
        "print(remove_noise(\"hello this is a sample text\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_noise(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "    return filtered_text\n",
        "\n",
        "text = \"hello i am tania\"\n",
        "filtered_text = remove_noise(text)\n",
        "print(filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wvCL2aqt2yk",
        "outputId": "7a5e6c57-4d7c-43a6-c637-c360ba6275e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello tania\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_regex(input_text, regex_pattern):\n",
        "  urls=re.finditer(regex_pattern,input_text)\n",
        "  for i in urls:\n",
        "    input_text=re.sub(i.group().strip(),' ',input_text)\n",
        "  return input_text\n",
        "\n",
        "regex_pattern=\"#[\\w]*\"\n",
        "inp=input(\"enter the sentence w hashtag:\")\n",
        "op=remove_regex(inp,regex_pattern)\n",
        "print(\"input text is\",'/n',inp)\n",
        "print(\"after hashtag removal:\",'/n',op)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGX3GL6ryTKV",
        "outputId": "e0de4ae1-d663-464d-cf1f-97150b4a1de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the sentence w hashtag:hello #tania is #ok\n",
            "input text is /n hello #tania is #ok\n",
            "after hashtag removal: /n hello   is  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "word=input(\"enter a word: \")\n",
        "print (\"stemming:\")\n",
        "stem=LancasterStemmer()\n",
        "print(stem.stem(word))\n",
        "\n",
        "print('lemmatization:')\n",
        "lem=WordNetLemmatizer()\n",
        "print(lem.lemmatize(word,'v'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aA69QFRLSq1",
        "outputId": "37516c13-1b83-4b62-eb5a-0d11ac482078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter a word: university\n",
            "stemming:\n",
            "univers\n",
            "lemmatization:\n",
            "university\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "stemm = LancasterStemmer()\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "stemmed_words = [stemm.stem(word) for word in words]\n",
        "print(\"Stemming:\")\n",
        "print(' '.join(stemmed_words))\n",
        "\n",
        "lemmatized_words = [lem.lemmatize(word, 'v') for word in words]\n",
        "print(\"Lemmatization:\")\n",
        "print(' '.join(lemmatized_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "780_f2htN64k",
        "outputId": "6b3a7d50-08fe-4432-8328-4396c7f5cb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: you were living in chicago last year\n",
            "Stemming:\n",
            "you wer liv in chicago last year\n",
            "Lemmatization:\n",
            "you be live in chicago last year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text_data=input(\"enter text: \")\n",
        "text1=nltk.word_tokenize(text_data)\n",
        "print(\"part of speech tagging\")\n",
        "print(nltk.pos_tag(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t57rAk9KT0Np",
        "outputId": "f8e5ce74-a185-40f0-8ff0-6d99e63d7e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter text: hello it is i\n",
            "part of speech tagging\n",
            "[('hello', 'NN'), ('it', 'PRP'), ('is', 'VBZ'), ('i', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 2**"
      ],
      "metadata": {
        "id": "-ZAXrckZv1z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ3ejo273SzG",
        "outputId": "c5e10bad-5c23-428c-fa1e-ef4f5ec8e57f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdzm-6KeugsU",
        "outputId": "64e0a304-ff38-4dde-89c7-875a243ad197"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(corpus):\n",
        "    # Step 1: Sentence Tokenization\n",
        "    sentences = sent_tokenize(corpus)\n",
        "\n",
        "    # Step 2: Tokenization and Normalization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Convert to lowercase and remove punctuation\n",
        "        sentence = sentence.lower()\n",
        "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # Tokenize sentence\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # Remove stopwords\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "\n",
        "        processed_sentences.append(words)\n",
        "\n",
        "    return processed_sentences"
      ],
      "metadata": {
        "id": "1pr0KlIywUdj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Language models are used in many applications like speech recognition,\n",
        "machine translation, and predictive text. They are essential for understanding\n",
        "and generating natural language.\"\"\"\n",
        "\n",
        "# Preprocess the corpus\n",
        "preprocessed_corpus = preprocess_text(corpus)\n",
        "\n",
        "# Display preprocessed corpus\n",
        "for sentence in preprocessed_corpus:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxZKRAbewhjZ",
        "outputId": "390efa9c-7fbc-4074-f5b9-b9389eb1bbe6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['language', 'models', 'used', 'many', 'applications', 'like', 'speech', 'recognition', 'machine', 'translation', 'predictive', 'text']\n",
            "['essential', 'understanding', 'generating', 'natural', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate N-grams\n",
        "def generate_ngrams(tokenized_sentences, n):\n",
        "    ngrams = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        sentence = ['<s>'] * (n-1) + sentence + ['</s>']\n",
        "        ngrams += [tuple(sentence[i:i+n]) for i in range(len(sentence)-n+1)]\n",
        "    return ngrams\n"
      ],
      "metadata": {
        "id": "n-QLwquRwwTr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(ngrams, ngram_probabilities, n_minus_1_grams, vocab_size):\n",
        "    N = len(ngrams)\n",
        "    log_prob_sum = 0\n",
        "\n",
        "    # Convert n_minus_1_grams (unigrams in this case) to a dictionary with actual counts\n",
        "    n_minus_1_gram_dict = defaultdict(lambda: 0)\n",
        "    for n_minus_1_gram in n_minus_1_grams:\n",
        "        n_minus_1_gram_dict[n_minus_1_gram] += 1\n",
        "\n",
        "    for ngram in ngrams:\n",
        "        if ngram in ngram_probabilities:\n",
        "            prob = ngram_probabilities[ngram]\n",
        "        else:\n",
        "            # Laplace smoothing if n-gram is unseen\n",
        "            context = ngram[:-1]\n",
        "            prob = 1 / (n_minus_1_grams.get(context, 0) + vocab_size)\n",
        "        log_prob_sum += math.log(prob)\n",
        "\n",
        "    perplexity = math.exp(-log_prob_sum / N)\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "sVkEhSaHxh2U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate probabilities with Laplace smoothing\n",
        "def calculate_ngram_probabilities(ngrams, n_minus_1_grams, vocab_size, n):\n",
        "    ngram_counts = defaultdict(lambda: 0)\n",
        "    n_minus_1_counts = defaultdict(lambda: 0)\n",
        "\n",
        "    for ngram in ngrams:\n",
        "        ngram_counts[ngram] += 1\n",
        "\n",
        "    for n_minus_1_gram in n_minus_1_grams:\n",
        "        n_minus_1_counts[n_minus_1_gram] += 1\n",
        "\n",
        "    probabilities = {}\n",
        "\n",
        "    for ngram in ngram_counts:\n",
        "        # Extract the n-1 gram (context)\n",
        "        context = ngram[:-1]\n",
        "        probabilities[ngram] = (ngram_counts[ngram] + 1) / (n_minus_1_counts[context] + vocab_size)\n",
        "\n",
        "    return probabilities"
      ],
      "metadata": {
        "id": "R_MXEiWhxQl1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text= ['language', 'models', 'used', 'many', 'applications', 'like', 'speech', 'recognition', 'machine', 'translation', 'predictive', 'text']\n",
        "['essential', 'understanding', 'generating', 'natural', 'language']\n",
        "\n",
        "# Generate bigrams (N=2)\n",
        "bigrams = generate_ngrams(preprocessed_corpus, 2)\n",
        "\n",
        "# Generate unigrams (N=1) for context\n",
        "unigrams = generate_ngrams(preprocessed_corpus, 1)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab = set([word for sentence in preprocessed_corpus for word in sentence])\n",
        "vocab_size = len(vocab) + 1  # Add 1 for Laplace smoothing\n",
        "\n",
        "# Calculate bigram probabilities with Laplace smoothing\n",
        "bigram_probabilities = calculate_ngram_probabilities(bigrams, unigrams, vocab_size, 2)\n",
        "\n",
        "print(\"Bigram Probabilities:\")\n",
        "for bigram, prob in bigram_probabilities.items():\n",
        "    print(f\"{bigram}: {prob:.4f}\")\n",
        "\n",
        "# Calculate perplexity\n",
        "perplexity = calculate_perplexity(bigrams, bigram_probabilities, unigrams, vocab_size)\n",
        "\n",
        "print(f\"Bigram Model Perplexity: {perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yghlv6-Lxp7Y",
        "outputId": "4b1c8ced-4e37-4857-baa7-500f50215ed7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Probabilities:\n",
            "('<s>', 'language'): 0.1176\n",
            "('language', 'models'): 0.1053\n",
            "('models', 'used'): 0.1111\n",
            "('used', 'many'): 0.1111\n",
            "('many', 'applications'): 0.1111\n",
            "('applications', 'like'): 0.1111\n",
            "('like', 'speech'): 0.1111\n",
            "('speech', 'recognition'): 0.1111\n",
            "('recognition', 'machine'): 0.1111\n",
            "('machine', 'translation'): 0.1111\n",
            "('translation', 'predictive'): 0.1111\n",
            "('predictive', 'text'): 0.1111\n",
            "('text', '</s>'): 0.1111\n",
            "('<s>', 'essential'): 0.1176\n",
            "('essential', 'understanding'): 0.1111\n",
            "('understanding', 'generating'): 0.1111\n",
            "('generating', 'natural'): 0.1111\n",
            "('natural', 'language'): 0.1111\n",
            "('language', '</s>'): 0.1053\n",
            "Bigram Model Perplexity: 8.997071978174532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to predict the next word based on the current word and bigram probabilities\n",
        "def predict_next_word(current_word, bigram_probabilities, vocab):\n",
        "    # Collect possible next words and their probabilities\n",
        "    next_words_probs = {ngram[1]: prob for ngram, prob in bigram_probabilities.items() if ngram[0] == current_word}\n",
        "\n",
        "    if not next_words_probs:\n",
        "        # If no next words are found, randomly choose a word from the vocab\n",
        "        return random.choice(list(vocab))\n",
        "\n",
        "    # Choose the next word with the highest probability (deterministic)\n",
        "    next_word = max(next_words_probs, key=next_words_probs.get)\n",
        "\n",
        "    return next_word\n",
        "\n",
        "# Function to generate a sentence using bigram probabilities\n",
        "def generate_sentence(bigram_probabilities, vocab, max_length=10):\n",
        "    sentence = ['<s>']  # Start the sentence with the start token\n",
        "    current_word = '<s>'\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        next_word = predict_next_word(current_word, bigram_probabilities, vocab)\n",
        "        if next_word == '</s>':  # If the end token is predicted, stop the generation\n",
        "            break\n",
        "        sentence.append(next_word)\n",
        "        current_word = next_word\n",
        "\n",
        "    return ' '.join(sentence[1:])  # Remove the start token and return the sentence\n",
        "\n",
        "# Example corpus (already preprocessed)\n",
        "preprocessed_corpus = [\n",
        "    ['language', 'models', 'used', 'many', 'applications', 'speech', 'recognition'],\n",
        "    ['machine', 'translation', 'predictive', 'text'],\n",
        "    ['essential', 'understanding', 'generating', 'natural', 'language']\n",
        "]\n",
        "\n",
        "# Generate bigrams (N=2)\n",
        "bigrams = generate_ngrams(preprocessed_corpus, 2)\n",
        "\n",
        "# Generate unigrams (N=1) for context\n",
        "unigrams = generate_ngrams(preprocessed_corpus, 1)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab = set([word for sentence in preprocessed_corpus for word in sentence])\n",
        "vocab_size = len(vocab) + 1  # Add 1 for Laplace smoothing\n",
        "\n",
        "# Calculate bigram probabilities with Laplace smoothing\n",
        "bigram_probabilities = calculate_ngram_probabilities(bigrams, unigrams, vocab_size, 2)\n",
        "\n",
        "# Generate a sentence\n",
        "predicted_sentence = generate_sentence(bigram_probabilities, vocab, max_length=10)\n",
        "print(\"Predicted Sentence:\", predicted_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PmUeX3H49yo",
        "outputId": "d95dffdc-d50f-43b7-e5a5-1f56bec6feef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sentence: language models used many applications speech recognition\n"
          ]
        }
      ]
    }
  ]
}